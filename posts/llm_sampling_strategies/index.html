<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Trying simple tree-search techniques for LLM token sampling - ‚å®Ô∏èü§∑üèª‚Äç‚ôÇÔ∏èüì∑ </title><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=referrer content="no-referrer"><meta name=description content><meta property="og:site_name" content="‚å®Ô∏èü§∑üèª‚Äç‚ôÇÔ∏èüì∑"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:url" content="https://andys.page/posts/llm_sampling_strategies/"><meta property="og:title" content="Trying simple tree-search techniques for LLM token sampling"><meta property="og:image" content="https://andys.page/"><meta property="og:description" content><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Trying simple tree-search techniques for LLM token sampling"><meta name=twitter:description content><meta name=twitter:image content="https://andys.page/"><link rel=canonical href=https://andys.page/posts/llm_sampling_strategies/><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.5.2/css/bootstrap.min.css integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin=anonymous><link rel=stylesheet href=https://andys.page/css/custom.css><link rel="shortcut icon" href=https://andys.page/images/favicon.png><link href=https://andys.page/index.xml rel=alternate type=application/rss+xml title=‚å®Ô∏èü§∑üèª‚Äç‚ôÇÔ∏èüì∑></head><body><script data-goatcounter=https://andysalerno.goatcounter.com/count async src=//gc.zgo.at/count.js></script><script src=/js/shuffle-message.js defer></script><div class="mt-xl header"><div class=container><div class="row justify-content-center"><div class=col-auto><a href=https://andys.page/><h1 class=name>Andy, xor Andrew</h1></a><h8 class=tagline><i>Professional software developer / hobbyist street photographer <span class=bonusTagline></span><noscript>/ Respects your js-disabled browsing</noscript><span class=blinker></span></i></h8></div></div><div class="row justify-content-center"><ul class="nav nav-primary"><li class=nav-item><a class=nav-link href=/>Home</a></li><li class=nav-item><a class=nav-link href=/about/>About</a></li><li class=nav-item><a class=nav-link href=/readinglist/>Reading List</a></li><li class=nav-item><a class=nav-link href=https://github.com/andysalerno>GitHub</a></li><li class=nav-item><a class=nav-link href=/gallery/>Photo gallery</a></li><li class=nav-item><a class=nav-link href=https://www.flickr.com/photos/andysalerno/>Flickr</a></li><li class=nav-item><a class=nav-link href=/index.xml>Rss (there are dozens of us!)</a></li></ul></div></div></div><div id=myModal class=myModal><div class=flex-row><i class="myModal-content caption" id=modal-count>Click to view</i></div><div class="flex-row img-row"><div class=img-row-arrow-col><div class=flex-pusher onclick=goBackPrevImage()></div><div id=leftArrow class="myModal-content caption img-row-arrow" onclick=goBackPrevImage()>&lt;</div><div class=flex-pusher onclick=goBackPrevImage()></div></div><img class=myModal-content id=modal-img onclick=advanceNextImage()><div class=img-row-arrow-col><div class="caption close-button flex-pusher" onclick=closeModal()>X</div><div class=flex-pusher-08 onclick=advanceNextImage()></div><div id=rightArrow class="myModal-content caption img-row-arrow" onclick=advanceNextImage()>></div><div class=flex-pusher onclick=advanceNextImage()></div></div></div></div><script src=/js/modal.js></script><div class=content><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-lg-8"><h1 class="mx-0 mx-md-4">Trying simple tree-search techniques for LLM token sampling</h1><div class="meta-date mx-0 mx-md-4"><time>2023/11/26</time></div><div class=markdown><h1 id=trying-simple-tree-search-techniques-for-llm-token-sampling>Trying simple tree-search techniques for LLM token sampling</h1><figure><img src=assets/img2.jpg></figure><p>At its core, an LLM is a value function.</p><p>Given a state (i.e. the current context, or input text), it scores all possible next actions (i.e. tokens).</p><p>Therefore, it&rsquo;s pretty simple to imagine the task of sampling tokens as a state-space exploration problem, where we use a tree data structure to map out the state space and explore it to find high-scoring states.</p><p>For LLMs, the most common sampling technique is a naive greedy approach - simply take the next token with the highest score, every time.</p><p>Well, that&rsquo;s not entirely true - there are sampling techniques like <a href=https://huggingface.co/blog/how-to-generate>top_k, top_p, etc</a>, which are strategies used to probe the LLM down paths that it would not otherwise take if using a greedy approach. top_k will pick the top K scoring tokens, and sample from them using their scores as a probability distribution. top_p will sample from the top N tokens, where N is the smallest amount of tokens that reach a certain probability threshhold, and then pick from that set.</p><p>And yet, neither top_k nor top_p explore any further than the very next token. You could say they have &ldquo;depth&rdquo; value of 1, since they never look further down the state-space tree than one step.</p><p>What are some strategies for exploring further down the tree? Can we explore a bit deeper, and possibly find a future state that has a higher total score? A higher total score across all tokens should imply a better resonse from the LLM, right?</p><h2 id=beam-search>Beam Search</h2><p><a href=https://huggingface.co/blog/how-to-generate#beam-search>Beam search</a> is a strategy that is already supported by the Huggingface Transformers library.</p><p>Beam search will fire off multiple concurrent traversals down the tree, and when all N traversals reach a terminal state (i.e., the EOS, or end-of-stream token), it selects the beam that resulted in the highest score.</p><p>There are some more advanced flavors of beam search, <a href=https://huggingface.co/blog/constrained-beam-search>which you can read about here.</a></p><p>There&rsquo;s one major downside to beam search, however:</p><p>It uses <strong>far</strong> more memory, which is a problem when using consumer GPUs with paltry vram. To put this into perspective, when I use beam search with 3 beams on a 7B Mistral model, quantized to 4bits with AWQ, I&rsquo;ve seen my VRAM balloon to 18GB (out of the 24GB on my 3090). And this was with only a few hundred tokens!</p><figure><img src=assets/img3.jpg></figure><h2 id=a-naive-tree-search-approach>A Naive Tree Search Approach</h2><p>There&rsquo;s an alternative to beam search, which uses almost no extra memory, at the expense of more latency.</p><p>The concept is almost exactly the same as beam search, except instead of running the beams concurrently, we have a only single worker exploring the state space. This means we use almost no additional memory (again, at the expense of latency).</p><p>The implementation looks like something you&rsquo;d see out of your traditional programming interview, if the question was: &ldquo;Given a root node of a tree, find the subtree of length N that has the highest total value&rdquo;.</p><p>The implementation is so simple that I can show almost the entire thing right here:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00f>def</span> _evaluate(self, depth: int, max_depth: int, node: _Node) -&gt; Tuple[float, str]:
</span></span><span style=display:flex><span>    <span style=color:#00f>if</span> depth &gt; max_depth:
</span></span><span style=display:flex><span>        <span style=color:#00f>return</span> (node.score, node.text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green># give more weight to later states:</span>
</span></span><span style=display:flex><span>    <span style=color:green># modifier = math.log(depth + 2)</span>
</span></span><span style=display:flex><span>    modifier = 1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green># get the mappings of token: value for the top N next tokens</span>
</span></span><span style=display:flex><span>    <span style=color:green># this is where we actually get the token scores from the LLM</span>
</span></span><span style=display:flex><span>    next_token_scores = self._get_next_token_scores(node.text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    max_child_score = -float(<span style=color:#a31515>&#34;inf&#34;</span>)
</span></span><span style=display:flex><span>    max_child_sequence = <span style=color:#00f>None</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>for</span> k, v <span style=color:#00f>in</span> next_token_scores.items():
</span></span><span style=display:flex><span>        text = node.text + k
</span></span><span style=display:flex><span>        score = node.score + (modifier * v)
</span></span><span style=display:flex><span>        child = _Node(text, score=score, parent=node)
</span></span><span style=display:flex><span>        node.children.append(child)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        (child_best_score, child_best_sequence) = self._evaluate(
</span></span><span style=display:flex><span>            depth + 1, max_depth, child
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00f>if</span> child_best_score &gt; max_child_score:
</span></span><span style=display:flex><span>            max_child_score = child_best_score
</span></span><span style=display:flex><span>            max_child_sequence = child_best_sequence
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>return</span> (max_child_score, max_child_sequence)
</span></span></code></pre></div><p>In this approach, instead of picking the highest-scoring token and calling it a day, we explore the tree with a depth of N (configurable) and find the next N tokens that have the total highest score.</p><p>Unlike beam search, this approach uses almost no additional vram. And in my (totally subjective and unscientific) experiments, it produces higher quality output from the LLM.</p><p>I&rsquo;m not sure why tree search approaches like this are not more common in the world of LLM self-hosting, since it&rsquo;s so straightforward to implement, and the problem of searching a tree is a classic computer science problem with many known strategies and solutions, like the above.</p><figure><img src=assets/img4.jpg></figure><h2 id=results>Results</h2><p>If you&rsquo;re curious, here are some outputs, all for the exact same input prompt, but using the strategies described above.</p><h4 id=code>Code</h4><p>The code used to generate these results can be found in my repo here: &mldr;</p><p><strong>Prompt:</strong></p><p><em>sort the list [4, 5, 28, 12, 343, 29, 199, 404, 3, 101, 73] in ascending numeric order. Your answer must only include the sorted list, no additional text.</em></p><p>Note: the model used <a href=https://huggingface.co/TheBloke/openchat_3.5-AWQ>is openchat3.5, AWQ quantized from TheBloke</a>. The openchat3.5 chat template is used to convert the above prompt into the model&rsquo;s expected chat format.</p><p><strong>Expected result:</strong></p><p><code>[3, 4, 5, 12, 28, 29, 73, 101, 199, 343, 404]</code></p><h3 id=greedy-search-result>Greedy Search Result</h3><p><code>[3, 4, 5, 12, 28, 29, 343, 101, 199, 404, 343]</code></p><p>max vram usage: 4.6GB</p><p>answer: wrong ‚ùå</p><h3 id=beam-search-3-beams-result>Beam Search (3 beams) Result</h3><p><code>[3, 4, 5, 12, 28, 29, 101, 199, 343, 404]</code></p><p>max vram usage: 5.1GB</p><p>answer: wrong ‚ùå</p><h3 id=tree-search-depth3-topk3-result>Tree search (Depth=3, topK=3) Result</h3><p><code>[3, 4, 5, 12, 28, 29, 73, 101, 199, 343, 404]</code></p><p>max vram usage: 4.6GB</p><p>answer: correct ‚úÖ</p><h3 id=the-catch>The catch</h3><p>So what&rsquo;s the catch? Why not always use a naive tree search, if it gives better results and uses less vram?</p><p>Well, you&rsquo;re trading the memory for time, since we only have one worker processing the results at a time.</p><p>The &ldquo;tree search&rdquo; approach took roughly 26 seconds to generate on my 3090. The beam search took only around 5 seconds.</p><p>But, all is not lost. The tree search strategy is something I implemented in a few hours, leaving lots of obvious optimizations on the table. For example, I re-tokenize the full text every time we visit a node, even for text we have seen before. Beam search, on the other hand, is a first-class feature of the Transformers library. I&rsquo;m sure experts in LLM sampling could implement a tree search like mine with far better performance.</p><h2 id=closing-thoughts>Closing Thoughts</h2><p>Here is a list of some closing thoughts:</p><ul><li>Exploring deeper down the tree of tokens may help LLMs handle more complicated tasks, like sorting.</li><li>The naive approach I show above can be augmented to stop early, after reaching some confidence threshhold. It could also be adjusted to reduce the top_k value as the depth increases, to reduce how much computation is needed while still exploring deep through the state space.</li><li>Tree-search strategies could probably be implemented in ways that are cache-friendly, reducing the cost of exploration.</li><li>I have intentionally not touched upon the mysterious &ldquo;Q*&rdquo; project that has the crypto-bro-turned-AGI-experts crowd abuzz on Twitter, but it&rsquo;s not unthinkable that a much smaller model could be trained to guide the exploration of a larger model down promising token paths.</li><li>Given how simple and well-understood tree searching is, it seems kind of bizarre that the current status quo for LLM sampling is to sample from only the next step.</li></ul></div></div></div></div></div></body></html>